{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMLTtnDMyIVKYhEDXh5kr6v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Juuuuls/CPE311/blob/main/HoA_7_1_Data_Collection_and_Wrangling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Excercise 1**\n",
        "\n",
        "1. Read Each file in\n",
        "\n",
        "2. Add a column to each dataframe, called ticker, indicating the ticket symbol it is or (Apple's is AAPL, or example). This is how you look up a stock. Each file's name is also the ticker symbol, so be sue to capitalize it.\n",
        "\n",
        "3. .Append them together into a single dataframe.\n",
        "\n",
        "4. Save the result in a CSV file called faan.csv"
      ],
      "metadata": {
        "id": "ICgqAPk8Gw5_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "zBAimiTp9u7s"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "dataframes = []\n",
        "\n",
        "directory = '/content/'\n",
        "file_names = ['aapl.csv', 'amzn.csv', 'fb.csv', 'goog.csv', 'nflx.csv']\n",
        "\n",
        "for file_name in file_names:\n",
        "    ticker = os.path.splitext(file_name)[0].upper()\n",
        "\n",
        "    file_path = os.path.join(directory, file_name)\n",
        "\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    df['ticker'] = ticker\n",
        "\n",
        "    dataframes.append(df)\n",
        "\n",
        "combined_df = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "combined_df.to_csv('/content/faan.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Excecise 2**\n",
        "\n",
        "1. With faang, use type conversion to chane the date column into a datetime and the volume column into integers. Then , sort by date and ticke.\n",
        "\n",
        "2. Find the seven rows with the hihest value for volume.\n",
        "\n",
        "3. Right now, the data is somewhere between long and wide format. Use melt() to make it completely long format. Hint: date and ticker are our ID variables (they uniquely identify each row). We need to melt the rest so that we don't have seperate coolumns for open, high, low, close, and volume.\n"
      ],
      "metadata": {
        "id": "0OUUZHArHAZr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combined_df['date'] = pd.to_datetime(combined_df['date'])\n",
        "\n",
        "combined_df['volume'] = combined_df['volume'].astype(int)\n",
        "\n",
        "combined_df.sort_values(by=['date', 'ticker'], inplace=True)"
      ],
      "metadata": {
        "id": "ZYm9xWYZG_xP"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "highest_volume_rows = combined_df.nlargest(7, 'volume')\n",
        "\n",
        "print(highest_volume_rows)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAjfz1oxIwRw",
        "outputId": "299fdc4c-514e-4144-90ba-82709a2c6f9c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          date      open      high       low     close     volume ticker\n",
            "644 2018-07-26  174.8900  180.1300  173.7500  176.2600  169803668     FB\n",
            "555 2018-03-20  167.4700  170.2000  161.9500  168.1500  129851768     FB\n",
            "559 2018-03-26  160.8200  161.1000  149.0200  160.0600  126116634     FB\n",
            "556 2018-03-21  164.8000  173.4000  163.3000  169.3900  106598834     FB\n",
            "182 2018-09-21  219.0727  219.6482  215.6097  215.9768   96246748   AAPL\n",
            "245 2018-12-21  156.1901  157.4845  148.9909  150.0862   95744384   AAPL\n",
            "212 2018-11-02  207.9295  211.9978  203.8414  205.8755   91328654   AAPL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "id_vars = ['date', 'ticker']\n",
        "\n",
        "long_format_df = pd.melt(combined_df, id_vars=id_vars, var_name='metric', value_name='value')\n",
        "\n",
        "print(long_format_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmjEnHezI6XC",
        "outputId": "a45b3b75-22a0-4f19-9d29-b24ba4d0bbb4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           date ticker  metric         value\n",
            "0    2018-01-02   AAPL    open  1.669271e+02\n",
            "1    2018-01-02   AMZN    open  1.172000e+03\n",
            "2    2018-01-02     FB    open  1.776800e+02\n",
            "3    2018-01-02   GOOG    open  1.048340e+03\n",
            "4    2018-01-02   NFLX    open  1.961000e+02\n",
            "...         ...    ...     ...           ...\n",
            "6270 2018-12-31   AAPL  volume  3.500347e+07\n",
            "6271 2018-12-31   AMZN  volume  6.954507e+06\n",
            "6272 2018-12-31     FB  volume  2.462531e+07\n",
            "6273 2018-12-31   GOOG  volume  1.493722e+06\n",
            "6274 2018-12-31   NFLX  volume  1.350892e+07\n",
            "\n",
            "[6275 rows x 4 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Excecise 3**\n",
        "\n",
        "1. Usin web scaping, search for the list of hospitals, their address and contact information. Save the list in a new csv file, hospitals.csv.\n",
        "\n",
        "2. Using the generated hospitals.csv, convert the csv file into pandas dataframe. Prepare the data using the necessary preprocessing techniques.\n"
      ],
      "metadata": {
        "id": "Q3oxUxEcJCi0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = \"https://en.wikipedia.org/wiki/List_of_hospitals_in_Metro_Manila\"\n",
        "\n",
        "response = requests.get(url)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "    hospitals_data = []\n",
        "\n",
        "    hospital_entries = soup.find_all(\"div\", class_=\"elementor-element\")\n",
        "\n",
        "    for entry in hospital_entries:\n",
        "        name = entry.find(\"h3\", class_=\"elementor-heading-title\").text.strip()\n",
        "\n",
        "        address = entry.find(\"div\", class_=\"elementor-text-editor\").text.strip()\n",
        "\n",
        "        contact_info = entry.find(\"p\", class_=\"elementor-contact-info\").text.strip() if entry.find(\"p\", class_=\"elementor-contact-info\") else \"\"\n",
        "\n",
        "        hospitals_data.append({\"Name\": name, \"Address\": address, \"Contact Information\": contact_info})\n",
        "\n",
        "    csv_file_path = \"hospitals.csv\"\n",
        "    with open(csv_file_path, \"w\", newline=\"\", encoding=\"utf-8\") as csv_file:\n",
        "        writer = csv.DictWriter(csv_file, fieldnames=[\"Name\", \"Address\", \"Contact Information\"])\n",
        "        writer.writeheader()\n",
        "        writer.writerows(hospitals_data)\n",
        "\n",
        "    print(f\"The list of hospitals has been scraped and saved to {csv_file_path}\")\n",
        "else:\n",
        "    print(\"Failed to retrieve data from the website.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oGI2AnwUO1oN",
        "outputId": "d9b0ec66-af41-4fce-80ec-ace28f7e8a41"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The list of hospitals has been scraped and saved to hospitals.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"hospitals.csv\")\n",
        "\n",
        "print(\"Original DataFrame:\")\n",
        "print(df.head())\n",
        "\n",
        "df['Name'] = df['Name'].str.strip()\n",
        "df['Address'] = df['Address'].str.strip()\n",
        "df['Contact Information'] = df['Contact Information'].str.strip()\n",
        "\n",
        "print(\"\\nPreprocessed DataFrame:\")\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "7EmZpAnWPRMy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}